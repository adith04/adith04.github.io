<html>
	<head>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <meta name="description" content="">
        <meta name="author" content="">
    
        <title>Akshay Raman - Profile</title>
    
        <!-- Bootstrap core CSS -->
        <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    
        <!-- Custom fonts for this template -->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
        <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
        <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
        <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">
    
        <!-- Custom styles for this template -->
        <link href="css/resume.min.css" rel="stylesheet">
    
      </head>
	<body class="is-preload">

		<!-- Header -->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
              <span class="d-block d-lg-none">Start Bootstrap</span>
              <span class="d-none d-lg-block">
                <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
              </span>
            </a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
              <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
              <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#about">About</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#experience">Experience</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#education">Education</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#projects">Projects</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#skills">Skills</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#interests">Interests</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link js-scroll-trigger" href="https://akshayra97.github.io/index.html#awards">Awards</a>
                </li>
              </ul>
            </div>
          </nav>
            
        <!--Main-->    
        <div id="main">

            <!--License Project-->
            <section id="license" class="resume-section p-3 p-lg-5 d-flex flex-column">
                <div class="my-auto">
                  <h2 class="mb-5">Autonomous License Plate Detection</h2>
                  <p>Over the years, the traditional parking experience has remained largely unchanged: exhibiting outdated practices that rely 
                    heavily on manual ticketing systems. The conventional ticketing system necessitates physical tickets and manual payments at 
                    pay stations, causing inconveniences and inefficiencies. Automatic License Plate Detection (ALPD) can revolutionize the parking 
                    industry by eliminating all these manual components, providing a fully automated and streamlined parking experience in any 
                    parking scenario.</p>
                  <p class="mb-0">The projects seeks to develop an efficient ALPD system by combining Machine Learning (ML) and Computer Vision (CV) techniques, 
                    addressing the prevalent inaccuracies observed in many current solutions that solely depend on either ML or CV. 
                    The objective was to establish an optimized ML pipeline for ALPD using various machine learning models aided with pretrained 
                    networks and computer vision techniques to find the optimal Machine Learning (ML) pipeline for ALPD. </p>
                        <div class="row">
                            <div class="img-fluid img-profile rounded-circle mx-auto mb-2">
                                <h3><span class="image center"><img src="img/plate_splitting.png" /> </span></h3>
                            </div>
                    </div>
                    <p>The task of detecting a license plate was divided into three fundamental components:</p>
                    <ol type="1">
                        <li>Identifying the license plate bounding box</li>
                        <li>Isolating the characters</li>
                        <li>Predicting the characters</li>
                    </ol>
                      <p class="mb-0">The image above shows the first two steps of locating a the license plate bounding box and 
                        splitting the characters into multiple individual images. A pretrained YOLOv5 network was employed to discern 
                        the license plate amidst a dataset of vehicle images. Post identification, the license plate's bounding box was 
                        extracted for further processing to segment the characters it contained. Subsequently, contour techniques were 
                        utilized to detect letter edges within the license plate, segmenting all enclosed letter ‘shapes’ into 
                        individual images for character classification</p>
                        <br/>
                        <p>Three distinct classification algorithms, all trained on the EMNIST balanced dataset, were then 
                            assessed for character prediction: (1) K-nearest neighbor (KNN), (2) feedforward neural network (FNN), 
                            and (3) convoluted neural network (CNN). An ablation study was conducted on each ML pipeline to determine 
                            optimal hyperparameters and regularization settings, evaluating each model's detection performance under 
                            various angles and lighting conditions (edge cases). </p>
                           
                            <table class="table">
                                <tr>
                                  <th>Model Type</th>
                                  <th>Prediction Type</th>
                                  <th>Accuracy (%)</th>
                                </tr>
                                <tr>
                                  <td rowspan = "2">KNN</td>
                                  <td>Testing Accuracy (EMNIST)</td>
                                  <td>75.5</td>
                                </tr>
                                <tr class="bold-line">
                                    <td>Testing Accuracy (License Plate)</td>
                                    <td>18.6</td>
                                  </tr>
                                    <tr>
                                  <td rowspan = "2">FNN</td>
                                  <td>Testing Accuracy (EMNIST)</td>
                                  <td>81.3</td>
                                </tr>
                                <tr class="bold-line">
                                    <td>Testing Accuracy (License Plate)</td>
                                    <td>82.0</td>
                                  </tr>
                                  <tr>
                                    <td rowspan = "2">CNN</td>
                                    <td>Testing Accuracy (EMNIST)</td>
                                    <td>90.6</td>
                                  </tr>
                                  <tr>
                                      <td>Testing Accuracy (License Plate)</td>
                                      <td>66.7</td>
                                    </tr>
                              </table>
                        <p class="mb-0">The table above highlights the final results of all three models. 
                            Contrary to initial expectations favoring CNN for its feature extraction capabilities, the 
                            study revealed that the FNN model outperformed others with a testing accuracy of 88%, against CNN (67%) 
                            and KNN (19%). The findings affirm that employing feedforward neural networks in tandem with computer 
                            vision techniques can effectively detect license plate characters in real time across diverse conditions 
                            with minimal computational resources.</p>
                    
                </div>
              </section>

              <!--Motion Project-->
            <section id="motion" class="resume-section p-3 p-lg-5 d-flex flex-column">
                <div class="my-auto">
                  <h2 class="mb-5">Live Motion Direction Estimation</h2>
                  <p>This project delves into the burgeoning field of live motion direction estimation, a pivotal aspect of 
                    computer vision with broad implications for emerging technologies. In an era where autonomous systems, 
                    such as self-driving vehicles, augmented reality, and advanced surveillance, are becoming increasingly 
                    prevalent, the ability to accurately discern the direction of motion from video feeds is crucial. 
                    This initiative aims to develop a robust system capable of detecting and classifying camera movement directions 
                    in real-time, leveraging advanced image processing and machine learning techniques. This technology has the 
                    potential to greatly enhance the accuracy and reliability of motion-based applications.</p>
                  <p class="mb-0">The success of the algorithm depended on comprehensive data collection and innovative feature extraction 
                    methods. The team compiled a rich dataset consisting of approximately 4000 images. These images, derived from 
                    video feeds, were transformed into frames and further processed into disparity maps for detailed analysis. 
                    The disparity maps were classified into five distinct motion categories: forward, backward, turning left, 
                    turning right, and stationary. </p>
                        <div class="row">
                            <div class="img-fluid img-profile rounded-circle mx-auto mb-2">
                                <h3><span class="image center"><img src="img/data_col_motion.gif" /> </span></h3>
                            </div>
                            <div class="img-fluid img-profile rounded-circle mx-auto mb-2">
                                <h3><span class="image center"><img src="img/change_frame.png" /> </span></h3>
                            </div>
                    </div>
                    <p>The above gif shows one of the platforms the motion data was collected on. To avoid bias, the data was 
                        collected on multiple different platforms to keep the models from overfitting to a certain type of motion. For feature extraction, two primary methods were employed. The first involved analyzing changes in pixels over 
                        time by processing every fifth frame, providing insights into the fundamental aspects of frame change. The 
                        second method utilized the CV2.stereoSGBM_create function to generate disparity maps from grayscale images,
                         aiding in the detection of pixel differences or motion between stereo images.
                    </p>
                    <div class="row">
                        <div class="img-fluid img-profile rounded-circle mx-auto mb-2">
                            <h3><span class="image center"><img src="img/disparitymap.png" /> </span></h3>
                        </div>
                </div>
                <p>The project employed a rigorous evaluation process to determine the most effective method for live motion 
                    direction estimation. This involved testing various machine learning models, each offering unique strengths 
                    in handling image data and motion classification. The following models were evaluated:
                </p>
                    <ol type="1">
                        <li>Naive Bayes Classifier: Achieved a baseline accuracy of 74.7%, serving as an initial benchmark for 
                            comparison with more complex models.</li>
                        <li>Support Vector Classification (SVC): Demonstrated high efficacy with a 93% accuracy rate, benefiting 
                            from its strength in high-dimensional space handling.</li>
                        <li>Linear Neural Networks: Scored an accuracy of 83%, offering insights into the performance of simpler 
                            neural network architectures in the task.</li>
                        <li>Convolutional Neural Networks (CNN): Emerged as the most effective, with an impressive 94.6% accuracy, 
                            showcasing its superior capability in image processing and feature extraction.</li>
                    </ol>
                      <p class="mb-0">Conclusively, the project underscores the superiority of CNN in live motion direction estimation, 
                        particularly in image processing applications. To improve further, 
                        enhancements such as data augmentation to address motion jitter can be applied to better predict. A demo of
                        the CNN classifier predicting live motion is given in the gif below. Overall, the 
                        findings point towards the potential of CNN in real-world 
                        applications requiring precise motion direction analysis from video feeds.  </p>
                        <div class="row">
                            <div class="img-fluid img-profile rounded-circle mx-auto mb-2">
                                <h3><span class="image center"><img src="img/live_data_gif.gif" /> </span></h3>
                            </div>
                    </div>
                </div>
              </section>
        </div> 